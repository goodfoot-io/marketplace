---
description: Identify negative complexity drivers in documents
---

<negative-complexity-drivers>
# Negative Complexity Drivers

## 1. Introduction 
This report catalogs *negative complexity drivers* ‚Äì instruction-level characteristics that degrade model accuracy, consistency, or token efficiency ‚Äì and organizes them into a clear taxonomy. Drawing on military and aviation communication standards, technical writing guidelines, cognitive load theory, and the latest 2024‚Äì2025 AI benchmarks, we identify common **failure patterns** in prompts and system instructions. Each category below describes how specific structural, linguistic, cognitive, or domain-specific issues in prompt design can confuse models, increase hallucinations, or waste context window space (without proposing fixes). The goal is to help prompt engineers and documentation authors recognize red-flag traits that undermine Claude 4‚Äôs instruction-following performance.

## 2. Structural Complexity Drivers (Layout & Organization) 
Certain **formatting and organizational choices** in prompts create ambiguity, overload the context, or lead to misreads by the model:

- **Buried or Dispersed Key Instructions:** If the main task or requirement is not stated up front and instead hidden in the middle or end of a lengthy prompt, the model may miss or misunderstand it. Military writing doctrine emphasizes *"putting the main point at the beginning"* of a directive (the BLUF principle) for clarity. Instructions that *bury the lead* force the model to use more tokens to infer the goal, increasing the chance of error. Claude 4, like human readers, performs best when the purpose is explicitly stated early on.

- **Long, Unbroken Blocks (Wall of Text):** Lengthy paragraphs without breaks or subheadings overload the model‚Äôs short-term memory. The U.S. Army writing standard limits most paragraphs to **under 10 lines** and sentences to ~15 words. Ignoring such structure ‚Äì e.g. giving a half-page instruction with no bullet points or sections ‚Äì creates a dense ‚Äúwall of text.‚Äù This *visual density* is a negative driver: it hinders scanning for key points and can cause the model to overlook or forget details, analogous to how humans struggle with unstructured text.

- **Inconsistent or Shallow Heading Hierarchy:** Prompts with headings or bullet lists that are poorly organized (e.g. skipping levels, or mixing unrelated items under one heading) reduce coherence. Technical communication standards (ISO/IEC/IEEE 26514) stress a logical document hierarchy; when this is absent or inconsistent, users misinterpret relationships between sections. In an LLM context, a disordered or flat structure can confuse the model‚Äôs understanding of context. For example, a system prompt that jumps abruptly between topics without transitions or uses headings inconsistently can lead Claude to merge or ignore sections erroneously.

- **Multiple Topics in One Instruction:** An instruction step that tries to cover *too many actions or topics at once* tends to fail. High-reliability domains avoid ‚Äúoverloaded‚Äù steps ‚Äì e.g. a **checklist item** should contain one actionable item, not several, to ensure nothing is missed . Likewise, if a single prompt sentence says ‚ÄúDo X and Y and Z,‚Äù the model might execute some parts and neglect others. Such multi-topic commands dilute focus and behave like run-on orders that field manuals warn against (orders must be *‚Äúclear and sufficiently brief to be understood, yet complete‚Äù* ‚Äì too much in one sentence creates ambiguity).

- **Scattered Constraint Modifiers:** When important constraints or conditions are **spread far apart** in the prompt (e.g. a requirement mentioned in an early paragraph and another related caveat much later), the model may not associate them correctly. Cognitive research on *split-attention* shows performance drops when readers must mentally integrate distant pieces of information. In a prompt, if formatting choices force the model to ‚Äújump around‚Äù the context to gather all conditions (for instance, a critical note in a footnote or an appendix of the prompt), this extraneous navigation load can cause omissions or the need to re-read tokens. Effective prompts cluster relevant conditions together; their absence is a negative pattern that increases token usage and error.

- **Lack of Sequencing or Logical Flow:** **Non-sequential instructions** (out-of-order steps) breed confusion. In military orders, information is given in a standard sequence (Situation ‚Üí Mission ‚Üí Execution ‚Üí etc.) to maintain a shared mental picture . Deviating from a logical order ‚Äì e.g. presenting execution details before stating the mission or giving background after the task ‚Äì can make an LLM ‚Äúlose the plot.‚Äù The model might mis-assign what action corresponds to which context. A structured flow is critical; when prompts jump around arbitrarily or present steps in an illogical order, it mirrors the ambiguity of a jumbled operations order that *‚Äúfails to enable each subordinate to arrange his unit accordingly‚Äù*. Such loss of **narrative flow** forces the model to interpret context on its own, often incorrectly.

- **Redundant and Repetitive Segments (vs. Valuable Structured Repetition):** True redundancy ‚Äì providing the same instruction or detail multiple times without added value ‚Äì can *increase* confusion. This triggers the "**redundancy effect**" in cognitive load theory, where the learner (or model) wastes effort parsing duplicate information. Claude might give disproportionate weight to a repeated phrase or become uncertain if the re-stated instructions aren't verbatim. For instance, repeating a requirement in slightly different words ("summarize concisely" vs. "provide a brief summary") could make the model wonder if two distinct outputs are needed.

  **However, structured repetition is fundamentally different from redundancy.** When similar formatting is used to create comprehensive catalogs, taxonomies, or reference guides (e.g., a collection of distinct patterns, each following the same template structure), this is **not** a negative complexity driver. Such structured repetition serves critical functions:
  - **Pattern Recognition**: Consistent formatting helps readers quickly identify and compare different instances
  - **Comprehensive Coverage**: Complete taxonomies ensure no edge cases are missed
  - **Reference Value**: Users can quickly scan for the specific pattern they need
  - **Learning Aid**: Similar structure reduces cognitive load when learning multiple related concepts
  
  Examples of valuable structured repetition include: error pattern catalogs, design pattern collections, test case suites, command references, and diagnostic flowcharts. The key distinction: each instance must provide unique informational value despite similar formatting. Unnecessary restatement wastes token budget, but comprehensive reference materials justify their length through completeness.

**Why these structural issues degrade performance:** They impose extra work on the model‚Äôs context processing. A *well-structured prompt* provides a clear roadmap of the task; a poorly structured one forces the model to infer the hierarchy and relevance of each part. The result is often increased **hallucination** (filling gaps with assumptions) or **determinism loss** (inconsistent outputs on re-run, as the model may latch onto different parts each time). In short, messy layout = messy model behavior. High-reliability fields learned long ago that standardized, *brief, and well-organized* instructions reduce human error ‚Äì the same holds for prompting AI.

## 3. Linguistic Complexity Drivers (Wording & Clarity) 
The language style of a prompt can introduce ambiguity and cognitive strain. Several **wording patterns** are known to reduce comprehension or compliance, and they similarly confuse LLMs:

- **Passive Voice and Opaque Agency:** Instructions written in passive voice (e.g. ‚Äúthe data *should be analyzed*‚Äù) obscure *who* is supposed to do what . The Army‚Äôs writing doctrine flatly states that passive constructions impede clear communication, mandating active voice (‚ÄúYou *analyze the data*‚Äù) to assign responsibility . For LLMs, a passive instruction can lead to uncertainty: the model might not identify whether *it* should perform the action or if it‚Äôs describing a state. Research in psycholinguistics finds passive sentences are harder to parse and recall, leading to more errors than active equivalents . Thus, pervasive passive voice in prompts is a negative driver ‚Äì it muddies the intended action and can result in the model responding tangentially or failing to act altogether.

- **Ambiguous Modals and Weak Verbs:** Phrases that **hedge or weaken** the directive ‚Äì e.g. ‚Äúyou **should perhaps** do X,‚Äù ‚Äúit **might be good** to include Y‚Äù ‚Äì introduce ambiguity. In plain-language standards, **‚Äúmust‚Äù vs. ‚Äúshould‚Äù** is a crucial distinction: ‚Äúmust‚Äù conveys a requirement, whereas ‚Äúshould‚Äù suggests a recommendation, and mixing them confuses the audience. If a prompt says ‚ÄúThe assistant *should* output a JSON,‚Äù Claude may treat it as optional and return prose instead. Similarly, imprecise verbs like ‚Äúhandle‚Äù or ‚Äúaddress‚Äù (instead of ‚Äúcalculate,‚Äù ‚Äúlist,‚Äù etc.) leave too much room for interpretation. Clarity suffers when instructions use vague or modal language ‚Äì the model might comply only partially or default to a safe interpretation. In critical domains (aviation, military), there is an aversion to ambiguous terms; for example, pilots avoid non-standard phrases precisely because they can be interpreted in multiple ways under stress. In prompts, every ‚Äúmaybe, could, ideally‚Äù is a potential source of nondeterministic output.

- **Excessive Jargon or Undefined Acronyms:** Using niche terminology or abbreviations without explanation forces the model to rely on its training guess ‚Äì which might be wrong. Plain-language doctrine advises defining acronyms on first use and favoring common words over jargon . When a prompt drops in specialized terms (‚ÄúApply PCA to the KPI per SOP for Q4‚Äù ‚Äì full of acronyms), it risks misinterpretation. Claude might know some acronyms but not all, or might expand ‚ÄúSOP‚Äù incorrectly. Underspecified jargon increases hallucination as the model may *invent* definitions or related content to fill gaps. Even if technically correct, jargon can increase the token usage (since the model might ‚Äúexplain‚Äù a term it recognizes as uncommon). Thus, unless the domain is well within the model‚Äôs expertise, **jargon without context is a complexity driver**. It mirrors the known issue of *not communicating in a shared vocabulary*, which in human teams leads to errors and is cautioned against in multi-service operations.

- **Nominalizations and Wordiness:** Turning verbs into abstract nouns (e.g. ‚Äú*implementation of the plan*‚Äù instead of ‚Äú*implement the plan*‚Äù) and other **wordy constructions** make instructions heavier. These forms often accompany passive voice and lead to longer sentences. Cognitive studies show that sentences with multiple abstract nouns and clauses demand more working memory to untangle . For an LLM, a convoluted sentence increases the likelihood of mis-parse. For example, *‚ÄúPrior to the initiation of execution of the analysis by you‚Ä¶‚Äù* is much harder to follow than *‚ÄúBefore you begin the analysis‚Ä¶‚Äù*. Such linguistic complexity might cause Claude to miss temporal cues or mis-assign the action. The **Federal Plain Language Guidelines** explicitly warn against unnecessary complexity ‚Äì using simple, active phrasing improves comprehension and reduces error rates. In prompts, failure to do so (bloated, academic-style instructions) is counterproductive, often requiring the model to use more tokens to ‚Äúthink through‚Äù the language itself rather than the content.

- **Ambiguous Pronouns and References:** Unclear referents (e.g. ‚ÄúSend the report to **them** when **it** is ready‚Äù with multiple possible referents for ‚Äúthem‚Äù or ‚Äúit‚Äù) can confuse an AI just as they do humans. Unlike humans, the AI can‚Äôt ask for clarification, so it may guess ‚Äì a pathway to mistakes. This is analogous to poorly written procedure steps in which pronouns cause confusion about what object to use (a known documentation bug in high-risk fields). If a system prompt or user prompt uses pronouns without clear antecedents, Claude might latch onto an incorrect assumption from earlier text, leading to a wrong action. **Clarity in references** is key; its absence is a negative trait. As one military writing guide notes, *‚Äúensure each paragraph is coherent; pronouns should clearly refer to the intended antecedent‚Äù*. Violating this principle in LLM prompts can yield indeterminate outputs (the model‚Äôs answer might subtly shift if its internal resolution of a pronoun changes).

- **Double Negatives and Conditional Caveats:** Complex negations (e.g. ‚ÄúDo not fail to include‚Ä¶‚Äù) and stacked conditions make it easy for the model to slip. A prompt that says ‚ÄúAvoid not addressing X‚Äù is grammatically confusing ‚Äì does it mean ‚Äúaddress X‚Äù plainly? Ambiguously negated phrasing increases the risk of the model misunderstanding the requirement (possibly doing the opposite). In cognitive terms, negation adds processing load and is prone to errors in both human and AI interpretation. Similarly, instructions loaded with conditional clauses (‚ÄúIf A, unless B, then do C‚Äù) may be handled incorrectly if the model misses one condition while focusing on another. Each extra clause is a chance for a **misparse or dropped clause** in generation. Clarity suffers greatly with such constructs, which is why simplified logical structures or bullet lists are recommended in technical writing instead of burying logic in one sentence.

**In summary,** linguistic pitfalls revolve around *ambiguity and density*. Good instructions use straightforward, active language; bad ones use convoluted or equivocal wording. Claude‚Äôs compliance depends on unambiguous cues ‚Äì any fuzziness in language (hedging, passive voice, legalese, or run-on syntax) can degrade determinism and accuracy. Empirical studies confirm that *‚Äúpoor writing drives processing difficulty‚Äù* more than complex content itself . Prompts full of these negative traits thus invite errors or require extra tokens as the model ‚Äúthinks‚Äù its way through the confusion.

## 4. Cognitive Load Drivers (Excessive Mental Demand) 
Even when a prompt is structurally and linguistically sound, it can overwhelm an LLM (or a human reader) by imposing **extraneous cognitive load**. Key factors that increase cognitive strain in prompts include:

- **Split Attention Requirement:** This occurs when the prompt‚Äôs format forces the model to divide its attention between multiple sources or parts to understand the instructions. In humans, classic examples are text that must be read alongside a separate diagram, causing mental back-and-forth. In Claude‚Äôs case, a split-attention prompt might present a rule in one paragraph and an important caveat in a distant footnote, requiring the model to hold one in memory while parsing the other. Cognitive Load Theory shows that split attention *significantly hinders understanding* by overloading working memory. If the model has to reconcile far-flung pieces of context, it uses up more of its context window and is more likely to **forget or confuse** details. Thus, prompts that are not self-contained and coherent ‚Äì effectively asking the model to ‚Äúmulti-task‚Äù within the context ‚Äì are a negative design. They lead to higher token usage and greater chance of instructions being overlooked.

- **Extraneous Details and Irrelevant Information:** When a prompt contains a lot of *nice-to-know* background or unrelated text that isn‚Äôt necessary for the task, it creates **extraneous cognitive load**. The model must sift signal from noise. Human factors research indicates that any element not contributing to task goals will distract and reduce overall performance . For example, a system instruction that includes paragraphs of general advice or verbose role-play (beyond what‚Äôs needed for the specific query) can derail focus. Claude might spend effort producing a style or detail consistent with the extraneous info rather than excelling at the core task. In practical terms, unnecessary context not only risks hallucinations (the model might incorporate those random details into the answer), but also wastes tokens. High **mental demand** with low relevance is a known recipe for error in user interfaces and procedures. In prompts, it manifests as the model losing track of the actual question amidst the clutter.

- **High Information Density (Intrinsic Load) Without Chunking:** Some tasks are inherently complex ‚Äì e.g. a prompt might involve multiple pieces of data or a multi-step logical problem. If such intrinsically heavy content is presented without any scaffolding (no bullet points, no numbering, no step-by-step breakdown), the intrinsic cognitive load can exceed the model's effective capacity. Cognitive load theory differentiates *intrinsic load* (complexity of the material itself) from extraneous load; good design tries to **segment or scaffold** high intrinsic load material. In a prompt, failing to break a complex task into smaller steps or clear sub-tasks forces the model to hold a lot of details in working memory at once. This often results in the model dropping some constraints or steps when generating an answer ‚Äì a direct parallel to human error when overwhelmed by too much information simultaneously. For instance, a single prompt paragraph describing a complicated scenario with many variables and then asking for an analysis in one go is a likely negative driver: the model might focus on the most salient few details and ignore others, or get parts of the scenario wrong (hallucinating facts that "fill in" for forgotten ones). Note: A comprehensive set of patterns or examples that are clearly organized does not constitute problematic density if each serves a distinct purpose ‚Äì the organization itself provides the necessary scaffolding.

- **Redundant or Conflicting Objectives:** If a prompt gives **conflicting goals** or criteria that compete, it increases mental effort as the model tries to reconcile them. An example is instructing, ‚ÄúProvide a detailed explanation, but keep it concise.‚Äù This puts the model in a double bind: ‚Äúdetailed‚Äù vs ‚Äúconcise‚Äù are opposing pulls with no guidance on priority. In human psychology, such competing objectives cause confusion and stress ‚Äì the NASA Task Load Index (TLX) includes *frustration* as a factor that rises when people face contradictory demands. For Claude, ambiguous optimization targets can lead to unpredictable outputs depending on which side of the trade-off it prioritizes on a given run. Another example: ‚ÄúBe creative but do not deviate from the facts.‚Äù The model might err on one side (either producing dull factual output or hallucinating creative additions) because balancing these adds cognitive load. **Competing instructions in the same block** are thus a negative complexity driver. Ideally, instructions would clarify precedence (e.g. ‚Äúabove all, facts are paramount ‚Äì only add creativity if it doesn‚Äôt introduce fiction‚Äù), but without that, the model is left to its own devices.

- **Lack of Modular Structure (Chunking):** Closely related to the above, if a prompt does not chunk information into digestible pieces, it fails to utilize the model‚Äôs strength in understanding structured input. Research in instructional design shows *chunking information* reduces working memory burden and errors . In prompt terms, chunking could mean using lists, numbering, or separate messages to delineate parts of a task. A negative pattern is a monolithic prompt that blends context, instructions, and examples all together. This all-in-one approach makes it hard for the model to distinguish which sentences are background versus which are explicit directives. The result can be misunderstandings (e.g., the model might treat an example as an actual user request, or vice versa). Essentially, a lack of clear separation between context and task in the prompt increases cognitive load and error likelihood. It‚Äôs akin to an unchecked *‚Äúcontinuous text‚Äù* style manual, which technical writers avoid because readers miss important cues when nothing stands out.

In sum, these cognitive-load factors make prompts **harder to process**. A hallmark of good instruction (whether for a pilot, a surgeon, or an AI) is that it minimizes unnecessary mental work so the focus can be on the task. Negative drivers like extraneous info, split attention, and unchunked complexity all do the opposite ‚Äì they squander part of Claude‚Äôs finite attention on managing the prompt itself. Empirical cognitive studies consistently show that when instruction design is poor, *accuracy and speed plummet* as working memory is overwhelmed . With LLMs, this translates to inefficient use of the context window and increased chances of mistakes or omissions in the output.

## 5. High-Reliability Domain Pitfalls Mapped to Prompts 
Lessons from **military, aviation, and clinical** domains ‚Äì where communication can be life-critical ‚Äì highlight specific failure modes in instructions. Many of these map closely to pitfalls in LLM prompt design:

- **Lack of Standard Phraseology:** In aviation and military comms, everyone adheres to standardized phrases (e.g. ‚Äústand by‚Äù, ‚Äúaffirmative‚Äù) to avoid miscommunication. Deviating from these norms is known to cause errors or delays. For example, using a casual or non-standard phrase over the radio can lead to hesitation as the receiver interprets it, wasting time . In prompting, an analogy is using an inconsistent style or format. If one prompt says ‚ÄúList the steps:‚Äù and another similar prompt says ‚ÄúCould you perhaps enumerate how one might proceed,‚Äù the model may not consistently recognize they‚Äôre the same type of instruction. **Unstandardized phrasing** ‚Äì i.e. not keeping a consistent prompt style for similar tasks ‚Äì is a negative driver for determinism. It‚Äôs akin to switching jargon in the cockpit; the model might ‚Äúhesitate‚Äù (in effect, yield different outputs or require more tokens to respond). Consistency and predictable cue words help models latch onto the task more reliably. An authoritative example is the Multi-Service Brevity Codes manual, which exists precisely because brevity and uniform terms prevent confusion in joint operations. Prompt designers similarly benefit from a consistent vocabulary; its absence injects uncertainty.

- **Overloaded Communications (Too Much, Too Fast):** The concept of *overload* is well-documented in high-risk fields. A field commander who rattles off a long, complex order in one breath risks subordinates missing parts of it ‚Äì hence the need for concise, chunked orders . In the cockpit, overloaded radio calls have led to missed critical information when pilots were saturated with tasks. When mapping this to LLM prompts: a single message that tries to do everything (set context, specify format, enumerate multiple constraints, give examples, and ask the question all at once) is an overloaded communication. It‚Äôs prone to *parts of the instruction being missed or ignored*. Indeed, long prompts with many constraints often see the model satisfy the first few and neglect later ones (a phenomenon noted in recent LLM evaluations ). Just as humans use techniques like read-backs and step-by-step directives to cope with overload, an AI prompt should ideally be broken into manageable parts. If it isn‚Äôt, it mirrors known human failure modes under high workload: tasks are dropped or completed incorrectly when the **message density** is too high. Thus, monster one-shot prompts are a negative driver ‚Äì they may exceed Claude‚Äôs internal capacity to faithfully execute every part.

- **Missing Acknowledgment or Feedback Loops:** In crew resource management, one principle is to have call-and-response or confirmation of critical instructions (‚ÄúGear down?‚Äù ‚ÄúGear down confirmed.‚Äù). Without this, misunderstandings go unchecked. Obviously an AI can‚Äôt ask for clarification mid-prompt (unless designed to), but a related pitfall is **assuming the model has certain knowledge or context without confirming it**. For instance, a system instruction might assume the assistant knows it should not produce disallowed content because of an earlier policy statement ‚Äì but if that policy is far back in the system prompt, the model might not apply it unless reminded. This is analogous to not confirming that a team member heard the plan correctly. A negative pattern is when prompts rely on implicit shared context that isn‚Äôt actually reinforced. The WHO surgical checklist was introduced largely to force teams to speak up and verify critical points (correct patient, procedure, etc.), after recognizing that *omissions and assumptions kill* . Similarly, in prompts, if a critical constraint is merely implied or not re-stated when relevant, Claude might ‚Äúassume all is well‚Äù and proceed incorrectly. The failure to explicitly surface important context or get a virtual acknowledgment (through the model‚Äôs own summary, for example) can lead to instruction failure. In short, **unconfirmed context or missing feedback** in instructions is a risk; it‚Äôs the prompt design equivalent of not having a co-pilot read back an altitude clearance.

- **Multi-Step Tasks Without Clear Order or Ownership:** High-reliability domains stress clear sequencing of tasks and assignment of roles. If an operating room checklist step contains multiple actions for different people (‚ÄúAnesthesiologist and surgeon both verify equipment and patient details‚Äù), confusion can occur about who does what ‚Äì so good checklists separate these concerns. In prompting, a similar pitfall is giving a list of instructions that are not clearly delineated or prioritized. For example, a system prompt might say: ‚ÄúYou are a coder and a reviewer. Fix the bug and explain the fix and review the code for style.‚Äù Here multiple roles and objectives are mixed. The model might focus on one role (fixing code) and neglect the other (review), or muddle them together. **Role confusion** and unclear division of labor within a prompt (especially with agentic models that have tools or personas) can degrade performance. Anthropic‚Äôs own guidance suggests giving Claude a clear role in system prompts for a reason ‚Äì a lack of a clear role or multiple competing roles in the instructions can cause the model to oscillate or produce erratic outputs. Thus, prompts should avoid the ‚Äúeveryone does everything‚Äù trap that real teams also avoid. When we see instructions that don‚Äôt specify *who* (model vs user vs tool) handles which part of a task, that‚Äôs a red flag for potential failure.

- **Loss of Shared Mental Model (Context Disconnect):** In team operations, everyone needs the same situational awareness. Orders often start with a situation summary precisely to align mental models . A known failure mode is when sub-units act on outdated or partial information because the orders didn‚Äôt convey the full context. In LLM terms, a prompt that is incomplete or assumes too much can cause the model to fill gaps with its own training priors. For example, if a financial analysis prompt doesn‚Äôt specify the region or timeframe and Claude assumes standard defaults, the result might conflict with user expectations. The **situational picture** wasn‚Äôt fully shared. Another case is when the system prompt context and the user prompt context diverge (perhaps due to a long chat history or an updated instruction that wasn‚Äôt reconciled with earlier ones). If the model doesn‚Äôt ‚Äúrealize‚Äù a change in context because instructions were not explicit, it‚Äôs akin to a crew not realizing the mission parameters changed ‚Äì a classic recipe for error. Maintaining a **consistent context within the prompt** is crucial; failure to do so (through oversight or poor updates in a conversation) is a complexity driver that can lead to hallucinations or irrelevant answers. The model is essentially acting on an outdated mental model. High-reliability practice would call for a formal update or restatement of context whenever it changes; prompts should do the same. When they don‚Äôt, we have the negative pattern of context drift.

These domain-inspired pitfalls underscore that many **instruction failures are predictable and preventable**. Whether it‚Äôs an airplane cockpit or a chat with Claude, unclear communication leads to mistakes. Military and clinical checklists evolved to eliminate exactly things like ambiguous wording, overloaded steps, missing confirmations, and unclear responsibilities . The presence of those traits in an LLM prompt strongly indicates a higher risk of the model erring or producing inefficient output. This cross-disciplinary convergence gives us confidence in tagging these as ‚Äúred flag‚Äù characteristics for prompt design.

## 6. Evidence from 2024‚Äì2025 AI Benchmarks (Complexity vs Performance) 
Contemporary research on LLM instruction-following provides empirical backing for the above failure modes. Recent benchmarks specifically stress-tested models like Claude 4 with complex or layered instructions, yielding insights into how **negative complexity drivers** impact performance:

- **Performance Degrades with Added Constraints:** *FollowBench* (ACL 2024) introduced prompts with an increasing number of fine-grained constraints (content requirements, style guides, format specs, etc.) . The finding: *‚Äúthe performance of all tested models declines substantially with an increase in difficulty level (number of constraints)‚Äù* . Even top models like GPT-4 or Claude could only reliably satisfy about 3 constraints simultaneously before failing additional ones . This confirms that instruction overload is real ‚Äì each extra rule or detail is a potential negative driver. The observation aligns with our analysis that multi-topic or overly prescriptive prompts cause omissions. Notably, certain constraint types were especially challenging: **situational context constraints** (adding lots of background info) and **example-driven constraints** (providing an example and requiring the model to follow its pattern) led to more failures . The ‚ÄúSituation‚Äù result highlights that extensive context can backfire (the model gets distracted or confused by background details), and the ‚ÄúExample‚Äù result suggests that embedding example data in the prompt can mislead the model if not very clear. Both are essentially instances of extraneous cognitive load and split attention hurting accuracy .

- **Mixed or Composite Instructions Reveal Deficiencies:** *ComplexBench* (NeurIPS 2024) evaluated instructions composed of multiple constraint types simultaneously (e.g. a query that requires factual correctness, a specific format, and a particular style all at once) . It identified *‚Äúsignificant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition.‚Äù* . In other words, when an instruction has different kinds of requirements that interact (the kind of scenario that often leads to conflicting or dense prompts), models struggle. For instance, a prompt might say: ‚ÄúAnswer this question using information from the text (knowledge constraint), in the form of a bullet-point list (format constraint), and in a humorous tone (style constraint).‚Äù Such composite prompts exhibited failure modes like the model satisfying one aspect (e.g. format) but not the other (dropping the humor, or vice versa). This is empirical confirmation of the **‚Äúcurse of multiple instructions‚Äù** . Each added dimension increases cognitive load and chance of conflict, aligning with our earlier point that competing or numerous objectives are a negative driver. ComplexBench‚Äôs taxonomy of 19 constraint dimensions also helps pinpoint what combinations are worst ‚Äì e.g., combining a knowledge-intensive task with a strict format was especially error-prone . This suggests that prompts which simultaneously demand domain reasoning and rigid formatting (a not uncommon scenario in system instructions) are likely to trip up Claude 4.

- **Stability Issues in Long Prompts:** *LIFBench* (2024/25) specifically looked at **long-context scenarios**, measuring how consistently models follow instructions as the input context grows in size . Findings from LIFBench indicate that as prompt length increases (spanning thousands of tokens), models like Claude become less stable in following the given instructions . They may start ignoring earlier instructions or crucial details ‚Äì a phenomenon akin to a human forgetting earlier parts of a long briefing. This supports the idea that long, unsegmented prompts (one of our structural negatives) are problematic. LIFBench also introduced an automated scoring for *stability*, highlighting that even if a model can handle short instructions well, its performance can drift when those instructions are embedded in a very long conversation or document. In practical terms: if a system prompt plus user prompt together are extremely lengthy, Claude‚Äôs likelihood of strict compliance drops. This is evidence for **context dilution** as a driver of errors ‚Äì important instructions drown in the sea of context if not reinforced. It underscores why keeping prompts concise and focused (to minimize unnecessary token load) is vital; long-winded instructions invite model lapses.

- **Knowledge vs Instruction Conflicts:** A 2024 study on *Knowledge-Conditioned Instruction Following (KCIF)* observed that LLMs often **struggle when instructions tell them to modify or filter their knowledge-based answer** . For example, if an instruction says ‚ÄúFrom the following facts, give an answer *without mentioning X*,‚Äù models frequently slip and mention X anyway, especially if X is highly relevant or salient in their knowledge. This reveals a failure mode where the model‚Äôs internal knowledge retrieval competes with the explicit instruction ‚Äì a kind of ‚Äúextraneous load‚Äù where the model is juggling two drives (truthfully output knowledge vs. obey formatting/content rules). Murthy et al. found that even simple modifying instructions (like ‚Äúanswer in one sentence‚Äù or ‚Äúdon‚Äôt use proper nouns‚Äù) are missed at notable rates when they go against the grain of the content . This empirically backs the notion that **ambiguous or secondary instructions** can be dropped. If the prompt doesn‚Äôt strongly emphasize a constraint, the model‚Äôs default behavior (e.g. giving a thorough answer including all facts) might override it. We can relate this to our observation that hedged or buried constraints are risky ‚Äì the model might simply ignore a softly phrased ‚Äúplease avoid X‚Äù if its knowledge prior deems X important. The KCIF results encourage prompt writers to assume that *any instruction which competes with salient knowledge needs to be extremely clear and perhaps repeated.* If not, it‚Äôs a prime point of failure.

- **‚ÄúCurse of Multiple Instructions‚Äù:** A 2025 report bluntly titled *‚ÄúLarge Language Models Cannot Follow Multiple Instructions at Once‚Äù*  reinforces many of the above points. The authors characterize a phenomenon where even state-of-the-art models break down when given too many or concurrent instructions. They note that current LLMs are *inevitably prone to failure if asked to satisfy numerous constraints simultaneously*, calling it a fundamental limitation in present architectures . This is essentially a direct warning that piling on instructions ‚Äì especially those that might conflict or require trade-offs ‚Äì will likely yield incomplete compliance. It validates our cross-domain intuition: just as a single person can only effectively track a handful of tasks at once, an LLM has a limit to how many independent instructions it can juggle in one prompt. Beyond that, it will drop something important or muddle them. The ‚Äúcurse‚Äù paper suggests this is an ‚Äúextremely inconvenient fact‚Äù for practical LLM use, meaning prompt engineers must be mindful of not overloading a prompt. This research provides a strong theoretical backing to label **multi-instruction prompts as high risk** ‚Äì an accumulation of negative drivers often leads to outright failure.

In summary, the cutting-edge evaluations of 2024‚Äì2025 strongly corroborate the idea that **increasing complexity negatively impacts LLM performance** across the board. Each additional constraint, each extra paragraph of context, each simultaneous objective incrementally raises the chance of error or non-compliance. These studies give quantified evidence to what earlier sections described qualitatively. They also highlight specific trouble spots: contextual distractions, format/style requirements in tension with content, and sheer number of instructions are all common culprits when models go astray . For a Claude 4 Opus user, these findings translate to a simple mandate: keep prompts as straightforward as possible. The research shows that *even the best LLM today has a fragile ability to follow complex, lengthy, or conflicting instructions*. Recognizing the negative drivers in prompt design is the first step to avoiding them.

## 7. Examples of Problematic Instructions (Illustrative Excerpts) 
To concretize these abstract failure modes, here are **anonymized prompt snippets** that exhibit one or more negative complexity drivers. Each example is paired with a brief note on why it's problematic for Claude 4. (These are "what not to do" illustrations ‚Äì they highlight issues without showing the corrected prompt.)

**Note on Structured Examples**: While this section contains multiple examples following a similar format, this is an instance of valuable structured repetition ‚Äì each example illustrates a distinct failure mode. The consistent format (Prompt + Issue explanation) aids comprehension rather than creating redundancy.

- **Example 1: Buried Main Request** 
 *Prompt:* ‚ÄúHello Claude, welcome back! üôÇ Before we start, let me remind you of some background: [two paragraphs of context]. Anyway, I was thinking, if it isn‚Äôt too much trouble, perhaps you could assist me with something related to the data we discussed. The figures from Q3, which, as you know, might be slightly skewed due to seasonal effects, need to be reevaluated. We should ensure no anomalies. So, yeah, could you do that analysis when you have a chance?‚Äù 
 **Issue:** The actual task (‚Äúdo that analysis‚Äù of Q3 figures) is **hidden at the very end** and phrased vaguely. The prompt spends dozens of tokens on greetings and context and only indirectly asks for analysis. This exemplifies a buried lead and hedging language (‚Äúperhaps‚Ä¶ when you have a chance‚Äù) [Army 2013; PLAIN 2011]. Claude might produce a friendly chatty reply or summarize the background instead of performing the analysis, because the instruction was not prominent or clear. 

- **Example 2: Overloaded Multi-Task Directive** 
 *Prompt:* ‚Äú1. Summarize the following article in detail. 2. Translate the summary to French. 3. List five insights from the article. 4. Write a critique of the article‚Äôs methodology. 5. All outputs must be in a JSON format with clearly labeled sections in markdown.‚Äù 
 **Issue:** Five distinct tasks and an additional format requirement are given at once, with no prioritization. This is an extreme case of **overloading and competing objectives** . The model might execute the first few and ignore the rest, or mix up formats (e.g. forgetting to use JSON for the critique). Each item is complex on its own; together they‚Äôre very likely to exceed the model‚Äôs constraint-following capacity (as per FollowBench‚Äôs findings of failures beyond ~3 simultaneous constraints ). The lack of any guidance on how to balance detail vs. brevity (especially for the summary vs. insights vs. critique) adds to the cognitive load. 

- **Example 3: Ambiguous Reference and Passive Voice** 
 *Prompt:* ‚ÄúThe user‚Äôs query was processed by the system. It produced results that may be relevant, but they have not been reviewed. Provide an answer based on this, ensuring it is comprehensive. They should be notified if any assumptions were made.‚Äù 
 **Issue:** This instruction is in **passive voice** and has unclear pronouns. Who ‚Äúproduced results‚Äù ‚Äì the system or Claude? Who is ‚Äúthey‚Äù that should be notified ‚Äì the user or someone else? The request to ‚Äúprovide an answer based on this‚Äù is vague (based on what exactly ‚Äì the unreviewed results?). The passive construction ‚Äúwas processed‚Äù hides the agent, and ‚Äúthey should be notified‚Äù is an unclear obligation. Claude could easily misinterpret who does what, possibly resulting in an answer that speaks to the wrong audience or fails to notify the user. The prompt‚Äôs lack of clarity mirrors the kind of ambiguous instruction that in aviation CRM would prompt a ‚ÄúWho, me? Did you mean me to do that?‚Äù ‚Äì but the AI won‚Äôt ask, it will guess (risking error).

- **Example 4: Excessive Context and Split Attention** 
 *Prompt:* ‚Äú[System note: The assistant is a financial expert AI in 2025, with knowledge of global markets.] User: Given the data below, predict next quarter‚Äôs trends. [Attached is a 5-page financial report with dense tables and text] Also, recall the principles of fiscal policy we discussed earlier, and the latest news about central bank rates. Make sure to format the answer as specified in the company style guide provided in the PDF.‚Äù 
 **Issue:** This is a **context soup**. It provides a role, a broad request, a huge chunk of data, references another discussion (‚Äúdiscussed earlier‚Äù) and external info (‚Äúlatest news‚Äù), plus a formatting rule buried at the end referring to yet another document. The model has to *split attention* across multiple sources: the user query, the financial report, memory of fiscal policy principles, current events knowledge, and a style guide. This exemplifies extraneous load and split-source integration. Claude may either ignore some inputs (e.g. skip the style guide or forget the news) or produce a very lengthy response trying to satisfy everything, perhaps timing out or losing coherence. Each additional context source is a potential point of failure ‚Äì the prompt lacks focus. A human facing such instruction would likely be overwhelmed; the AI likewise may falter or use tokens inefficiently trying to juggle it all.

- **Example 5: Conflicting Tone and Content Guidance** 
 *Prompt:* ‚ÄúGenerate a response to the user that is extremely thorough and covers every detail. It should be written in a very simple manner that a child could understand. Use a formal academic tone throughout. Keep it brief.‚Äù 
 **Issue:** This prompt gives **conflicting directives** on style and length. ‚ÄúExtremely thorough‚Äù vs. ‚Äúkeep it brief‚Äù is an obvious contradiction, as is ‚Äúchild-understandable‚Äù vs. ‚Äúformal academic tone.‚Äù The model cannot satisfy all these simultaneously ‚Äì it‚Äôs forced into an internal conflict. This will likely yield an unpredictable compromise: e.g. a moderately detailed answer with semi-formal tone, which might not actually please any of the criteria. Worse, the evaluation of success is unclear: if the output is brief, it fails thoroughness; if it‚Äôs detailed, it fails brevity. This no-win situation is a classic *instruction failure mode*. It exemplifies why prompts should avoid incompatible goals. In essence, it‚Äôs setting Claude up to ‚Äúlose‚Äù no matter what it does, which is evident in inconsistent outputs across attempts.

Each of the above problematic examples embodies one or more negative complexity drivers identified earlier. Spotting these patterns in your own prompts can warn you that Claude 4 might misinterpret or partially comply. In the next section, we distill such warnings into a handy checklist.

## 8. Red-Flag Checklist for Prompt Complexity Pitfalls 

**Important**: This checklist itself demonstrates valuable structured repetition ‚Äì each item follows a similar format but identifies a distinct complexity driver. Do not confuse comprehensive checklists with redundancy.

When analyzing prompts or system instructions for Claude 4 Opus (or similar LLMs), use this **checklist of red flags** to identify potential negative complexity drivers. Each item describes a pattern that correlates with reduced model accuracy or efficiency:

- **Main point hidden or delayed?** ‚Äì Check whether the primary request appears at the beginning or is buried in the middle/end of the prompt. BLUF (Bottom Line Up Front) violations occur when the core task is not immediately clear.

- **Paragraphs overly long or unstructured?** ‚Äì Look for large blocks of text that lack breaks, lists, or emphasis. Wall-of-text formatting creates visual density that overloads the model's working memory.

- **Inconsistent or confusing organization?** ‚Äì Examine whether similar items are scattered across different sections, or if the prompt jumps between topics without logical transitions. Disorganized structure forces the model to infer relationships.

- **Multiple tasks jumbled together?** ‚Äì Count how many distinct outputs are requested in a single instruction (e.g. summarize *and* critique *and* translate). Multiple simultaneous tasks often exceed the model's constraint-following capacity.

- **Passive voice or unclear agents?** ‚Äì Identify instances of "is/was done" phrasing where the actor is ambiguous. Passive constructions obscure who should perform which action (the AI, the user, or a system).

- **Hedging or non-committal language?** ‚Äì Search for qualifier words like "maybe, if you can, sort of, perhaps, might, could" that weaken instructions. These modals signal optional rather than required actions.

- **Undefined jargon or acronyms?** ‚Äì Note any specialized terminology or abbreviations that you do not understand. Undefined terms force the model to guess meanings based on training data. 

- **Long, complex sentences?** ‚Äì Measure sentence complexity by counting clauses, parentheticals, and conjunctions. Sentences that are difficult to parse in one reading create processing overhead.

- **Ambiguous references (this/it/they)?** ‚Äì Identify pronouns or demonstratives that could refer to multiple antecedents. Unclear references create interpretation ambiguity.

- **Extraneous information included?** ‚Äì Evaluate whether all provided context directly contributes to the task. **Critical distinction**: Comprehensive catalogs, taxonomies, and reference materials (e.g., multiple patterns each describing a unique scenario) are NOT extraneous even when they follow similar formats. Such structured collections serve as valuable reference guides. Only flag as extraneous: background information that doesn't contribute to task completion, verbose role-play beyond task needs, or nice-to-know details that distract from the core objective.

- **Context requires cross-referencing?** ‚Äì Check if critical information is split across distant parts of the prompt (e.g., rules in one paragraph, exceptions in another). Split-source formatting forces the model to hold multiple pieces in working memory.

- **Conflicting or dual objectives?** ‚Äì Look for instructions that request incompatible outcomes (e.g. "be creative but don't add anything new" or "be thorough but brief"). Contradictory requirements create unresolvable tensions.

- **No clear sequence for multi-step tasks?** ‚Äì Assess whether multi-step processes are presented in a clear order or jumbled together. Unsequenced steps increase the likelihood of omissions or reordering.

- **Role or perspective confusion?** ‚Äì Examine prompts with multiple actors or viewpoints for clarity about who performs each action. Mixed roles without clear delineation lead to perspective errors.

- **Overly long overall prompt?** ‚Äì Measure the total prompt length in tokens or paragraphs. Extended prompts increase the risk of earlier instructions being forgotten or deprioritized.

- **Too many constraints or rules?** ‚Äì Count the number of distinct requirements (format, style, content, length, etc.). Research shows model performance degrades significantly beyond 3-4 simultaneous constraints.

- **Structured repetition mistaken for redundancy?** ‚Äì Before flagging repetitive content as problematic, verify it's true redundancy rather than a comprehensive catalog or taxonomy. Pattern collections, error catalogs, example suites, and reference guides with consistent formatting are valuable resources, not complexity drivers.
</negative-complexity-drivers>

<user-message>
$ARGUMENTS
</user-message>

# Task

Think deeply. Output a comprehensive list of negative complexity drivers in the documents the user specified. For each, identify the location(s) in the document and why it's a negative complexity driver. Do not suggest solutions for the negative complexity drivers at this time.

When analyzing repetitive content, apply this critical distinction:

**True Redundancy (Negative Complexity Driver)**:
- Same information repeated without added value
- Rephrasing identical concepts in slightly different words
- Circular explanations that don't advance understanding
- Multiple statements of the same requirement or constraint

**Structured Repetition (NOT a Negative Complexity Driver)**:
- Pattern catalogs where each entry describes a unique scenario
- Comprehensive taxonomies covering different cases
- Reference guides with consistent formatting
- Example collections where each illustrates a distinct concept
- Diagnostic matrices with multiple unique conditions
- Command references with similar syntax but different functions

**Key Test**: Ask "Does each repeated element provide unique informational value?" If yes, it's valuable structured repetition that aids comprehension and completeness. The similar formatting is a feature, not a bug ‚Äì it reduces cognitive load by providing predictable structure while ensuring comprehensive coverage.

Note: All documents will be prepended with the content of @CLAUDE.md which contains a variety of general-purpose information relevant to the document.

# Features to Ignore in Analysis
- **YAML frontmatter:** This metadata is processed by Claude Code and should not be considered part of the document content.

# Transformations Before Analysis
- **The !`echo '$AR''GUMENTS'` variable:** The string !`echo '"$AR''GUMENTS"'` is replaced with the text following a slash command. For example, in the slash command "/echo Foo" the first instance of the !`echo '"$AR''GUMENTS"'` string in `.claude/commands/echo.md` would be replaced with "Foo". The model understands this will be replaced with user input.
- **Embedded Bash commands such as !`echo -e '!\u0060pwd\u0060'`:** The stdout of embedded bash commands is included in the template. For example, the embedded bash command !`echo -e '!\u0060echo "Bar"\u0060'` would be replaced with "Bar". You should consider the stdout of embedded bash commands as part of the document content, but not the command itself.

# Additional Resources

- Claude Code Slash Commands: @documentation/claude-code-slash-commands.md
- Claude Code Subagents: @documentation/claude-code-subagents.md