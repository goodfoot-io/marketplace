# Negative Complexity Drivers in Claude 4 Opus Prompts & System Instructions

## 1. Introduction  
Advanced language models like **Claude 4 Opus** can falter when faced with overly complex or poorly structured instructions. This report catalogs *negative complexity drivers* ‚Äì instruction-level characteristics that degrade model accuracy, consistency, or token efficiency ‚Äì and organizes them into a clear taxonomy. Drawing on military and aviation communication standards, technical writing guidelines, cognitive load theory, and the latest 2024‚Äì2025 AI benchmarks, we identify common **failure patterns** in prompts and system instructions. Each category below describes how specific structural, linguistic, cognitive, or domain-specific issues in prompt design can confuse models, increase hallucinations, or waste context window space (without proposing fixes). The goal is to help prompt engineers and documentation authors recognize red-flag traits that undermine Claude 4‚Äôs instruction-following performance.

## 2. Structural Complexity Drivers (Layout & Organization)  
Certain **formatting and organizational choices** in prompts create ambiguity, overload the context, or lead to misreads by the model:

- **Buried or Dispersed Key Instructions:** If the main task or requirement is not stated up front and instead hidden in the middle or end of a lengthy prompt, the model may miss or misunderstand it. Military writing doctrine emphasizes *‚Äúputting the main point at the beginning‚Äù* of a directive (the BLUF principle) for clarity [oai_citation:0‚Ä°corpslakes.erdc.dren.mil](https://corpslakes.erdc.dren.mil/employees/pdfs/AR25-50.pdf#:~:text=b,Active%20voice%20writing%E2%80%94) [oai_citation:1‚Ä°sapperscribe.com](https://www.sapperscribe.com/post/on-style-army-writing-v-column-writing#:~:text=AR%2025,your%20main%20arguments%20as%20necessary). Instructions that *bury the lead* force the model to use more tokens to infer the goal, increasing the chance of error. Claude 4, like human readers, performs best when the purpose is explicitly stated early on [Army 2013].

- **Long, Unbroken Blocks (Wall of Text):** Lengthy paragraphs without breaks or subheadings overload the model‚Äôs short-term memory. The U.S. Army writing standard limits most paragraphs to **under 10 lines** and sentences to ~15 words [oai_citation:2‚Ä°corpslakes.erdc.dren.mil](https://corpslakes.erdc.dren.mil/employees/pdfs/AR25-50.pdf#:~:text=%281%29%20Use%20short%20words,all%20individuals%2C%20and%20so%20forth) [oai_citation:3‚Ä°sapperscribe.com](https://www.sapperscribe.com/post/on-style-army-writing-v-column-writing#:~:text=%281%29%20Use%20short%20words). Ignoring such structure ‚Äì e.g. giving a half-page instruction with no bullet points or sections ‚Äì creates a dense ‚Äúwall of text.‚Äù This *visual density* is a negative driver: it hinders scanning for key points and can cause the model to overlook or forget details, analogous to how humans struggle with unstructured text [Dept. of the Army 2013].

- **Inconsistent or Shallow Heading Hierarchy:** Prompts with headings or bullet lists that are poorly organized (e.g. skipping levels, or mixing unrelated items under one heading) reduce coherence. Technical communication standards (ISO/IEC/IEEE 26514) stress a logical document hierarchy; when this is absent or inconsistent, users misinterpret relationships between sections [ISO 2022]. In an LLM context, a disordered or flat structure can confuse the model‚Äôs understanding of context. For example, a system prompt that jumps abruptly between topics without transitions or uses headings inconsistently can lead Claude to merge or ignore sections erroneously.

- **Multiple Topics in One Instruction:** An instruction step that tries to cover *too many actions or topics at once* tends to fail. High-reliability domains avoid ‚Äúoverloaded‚Äù steps ‚Äì e.g. a **checklist item** should contain one actionable item, not several, to ensure nothing is missed [oai_citation:4‚Ä°newsletter.jamieleeclark.com](https://newsletter.jamieleeclark.com/p/optimising-communication#:~:text=Extraneous%20load%20%E2%80%93%20the%20unnecessary,cluttered%20slides%2C%20vague%20instructions). Likewise, if a single prompt sentence says ‚ÄúDo X and Y and Z,‚Äù the model might execute some parts and neglect others. Such multi-topic commands dilute focus and behave like run-on orders that field manuals warn against (orders must be *‚Äúclear and sufficiently brief to be understood, yet complete‚Äù* ‚Äì too much in one sentence creates ambiguity) [Marine Corps 2019].

- **Scattered Constraint Modifiers:** When important constraints or conditions are **spread far apart** in the prompt (e.g. a requirement mentioned in an early paragraph and another related caveat much later), the model may not associate them correctly. Cognitive research on *split-attention* shows performance drops when readers must mentally integrate distant pieces of information [Sweller 2011]. In a prompt, if formatting choices force the model to ‚Äújump around‚Äù the context to gather all conditions (for instance, a critical note in a footnote or an appendix of the prompt), this extraneous navigation load can cause omissions or the need to re-read tokens. Effective prompts cluster relevant conditions together; their absence is a negative pattern that increases token usage and error.

- **Lack of Sequencing or Logical Flow:** **Non-sequential instructions** (out-of-order steps) breed confusion. In military orders, information is given in a standard sequence (Situation ‚Üí Mission ‚Üí Execution ‚Üí etc.) to maintain a shared mental picture [oai_citation:5‚Ä°trngcmd.marines.mil](https://www.trngcmd.marines.mil/Portals/207/Docs/TBS/B2B2377%20Combat%20Orders%20Foundations.pdf#:~:text=O,as%20follows%3A%20Orientation%20I%20Situation) [oai_citation:6‚Ä°trngcmd.marines.mil](https://www.trngcmd.marines.mil/Portals/207/Docs/TBS/B2B2377%20Combat%20Orders%20Foundations.pdf#:~:text=Paragraph%20I%3A%20Situation%20The%20first,that%20must%20be%20disseminated%20to). Deviating from a logical order ‚Äì e.g. presenting execution details before stating the mission or giving background after the task ‚Äì can make an LLM ‚Äúlose the plot.‚Äù The model might mis-assign what action corresponds to which context. A structured flow is critical; when prompts jump around arbitrarily or present steps in an illogical order, it mirrors the ambiguity of a jumbled operations order that *‚Äúfails to enable each subordinate to arrange his unit accordingly‚Äù* [Marine Corps 2019, citing Marshall]. Such loss of **narrative flow** forces the model to interpret context on its own, often incorrectly.

- **Redundant and Repetitive Segments:** Surprisingly, providing the same instruction or detail multiple times in a prompt can *increase* confusion if not done carefully. While some redundancy can emphasize key points, extraneous repetition triggers the ‚Äú**redundancy effect**‚Äù in cognitive load theory ‚Äì the learner (or model) wastes effort parsing duplicate information [Sweller 2011]. Claude might give disproportionate weight to a repeated phrase or become uncertain if the re-stated instructions aren‚Äôt verbatim. For instance, repeating a requirement in slightly different words (‚Äúsummarize concisely‚Äù vs. ‚Äúprovide a brief summary‚Äù) could make the model wonder if two distinct outputs are needed. Unnecessary restatement also wastes token budget. Clarity suffers when prompts are not succinct; as one style guide quips, *‚Äúsuperfluous or trivial phrases weaken an order and create ambiguity‚Äù* [Marine Corps 2015].

**Why these structural issues degrade performance:** They impose extra work on the model‚Äôs context processing. A *well-structured prompt* provides a clear roadmap of the task; a poorly structured one forces the model to infer the hierarchy and relevance of each part. The result is often increased **hallucination** (filling gaps with assumptions) or **determinism loss** (inconsistent outputs on re-run, as the model may latch onto different parts each time). In short, messy layout = messy model behavior. High-reliability fields learned long ago that standardized, *brief, and well-organized* instructions reduce human error [oai_citation:7‚Ä°trngcmd.marines.mil](https://www.trngcmd.marines.mil/Portals/207/Docs/TBS/B2B2377%20Combat%20Orders%20Foundations.pdf#:~:text=affect%20their%20actions%2C%20and%20many,Our%20troops%20suffered%20much%20from) [oai_citation:8‚Ä°trngcmd.marines.mil](https://www.trngcmd.marines.mil/Portals/207/Docs/TBS/B2B2377%20Combat%20Orders%20Foundations.pdf#:~:text=delays%20involved%20in%20preparing%20long,good%20order%20is%20as%20much) ‚Äì the same holds for prompting AI.

## 3. Linguistic Complexity Drivers (Wording & Clarity)  
The language style of a prompt can introduce ambiguity and cognitive strain. Several **wording patterns** are known to reduce comprehension or compliance, and they similarly confuse LLMs:

- **Passive Voice and Opaque Agency:** Instructions written in passive voice (e.g. ‚Äúthe data *should be analyzed*‚Äù) obscure *who* is supposed to do what [oai_citation:9‚Ä°corpslakes.erdc.dren.mil](https://corpslakes.erdc.dren.mil/employees/pdfs/AR25-50.pdf#:~:text=e,instead%20of%20taking%20the%20action). The Army‚Äôs writing doctrine flatly states that passive constructions impede clear communication, mandating active voice (‚ÄúYou *analyze the data*‚Äù) to assign responsibility [oai_citation:10‚Ä°corpslakes.erdc.dren.mil](https://corpslakes.erdc.dren.mil/employees/pdfs/AR25-50.pdf#:~:text=d,passive%20construction%20occurs%20when%20the). For LLMs, a passive instruction can lead to uncertainty: the model might not identify whether *it* should perform the action or if it‚Äôs describing a state. Research in psycholinguistics finds passive sentences are harder to parse and recall, leading to more errors than active equivalents [Mart√≠nez et al. 2022]. Thus, pervasive passive voice in prompts is a negative driver ‚Äì it muddies the intended action and can result in the model responding tangentially or failing to act altogether.

- **Ambiguous Modals and Weak Verbs:** Phrases that **hedge or weaken** the directive ‚Äì e.g. ‚Äúyou **should perhaps** do X,‚Äù ‚Äúit **might be good** to include Y‚Äù ‚Äì introduce ambiguity. In plain-language standards, **‚Äúmust‚Äù vs. ‚Äúshould‚Äù** is a crucial distinction: ‚Äúmust‚Äù conveys a requirement, whereas ‚Äúshould‚Äù suggests a recommendation, and mixing them confuses the audience [PLAIN 2011]. If a prompt says ‚ÄúThe assistant *should* output a JSON,‚Äù Claude may treat it as optional and return prose instead. Similarly, imprecise verbs like ‚Äúhandle‚Äù or ‚Äúaddress‚Äù (instead of ‚Äúcalculate,‚Äù ‚Äúlist,‚Äù etc.) leave too much room for interpretation. Clarity suffers when instructions use vague or modal language ‚Äì the model might comply only partially or default to a safe interpretation. In critical domains (aviation, military), there is an aversion to ambiguous terms; for example, pilots avoid non-standard phrases precisely because they can be interpreted in multiple ways under stress [FAA 2004]. In prompts, every ‚Äúmaybe, could, ideally‚Äù is a potential source of nondeterministic output.

- **Excessive Jargon or Undefined Acronyms:** Using niche terminology or abbreviations without explanation forces the model to rely on its training guess ‚Äì which might be wrong. Plain-language doctrine advises defining acronyms on first use and favoring common words over jargon [oai_citation:11‚Ä°corpslakes.erdc.dren.mil](https://corpslakes.erdc.dren.mil/employees/pdfs/AR25-50.pdf#:~:text=%281%29%20Use%20short%20words,all%20individuals%2C%20and%20so%20forth). When a prompt drops in specialized terms (‚ÄúApply PCA to the KPI per SOP for Q4‚Äù ‚Äì full of acronyms), it risks misinterpretation. Claude might know some acronyms but not all, or might expand ‚ÄúSOP‚Äù incorrectly. Underspecified jargon increases hallucination as the model may *invent* definitions or related content to fill gaps. Even if technically correct, jargon can increase the token usage (since the model might ‚Äúexplain‚Äù a term it recognizes as uncommon). Thus, unless the domain is well within the model‚Äôs expertise, **jargon without context is a complexity driver**. It mirrors the known issue of *not communicating in a shared vocabulary*, which in human teams leads to errors and is cautioned against in multi-service operations [NATO 2020].

- **Nominalizations and Wordiness:** Turning verbs into abstract nouns (e.g. ‚Äú*implementation of the plan*‚Äù instead of ‚Äú*implement the plan*‚Äù) and other **wordy constructions** make instructions heavier. These forms often accompany passive voice and lead to longer sentences. Cognitive studies show that sentences with multiple abstract nouns and clauses demand more working memory to untangle [Mart√≠nez et al. 2022]. For an LLM, a convoluted sentence increases the likelihood of mis-parse. For example, *‚ÄúPrior to the initiation of execution of the analysis by you‚Ä¶‚Äù* is much harder to follow than *‚ÄúBefore you begin the analysis‚Ä¶‚Äù*. Such linguistic complexity might cause Claude to miss temporal cues or mis-assign the action. The **Federal Plain Language Guidelines** explicitly warn against unnecessary complexity ‚Äì using simple, active phrasing improves comprehension and reduces error rates [PLAIN 2011]. In prompts, failure to do so (bloated, academic-style instructions) is counterproductive, often requiring the model to use more tokens to ‚Äúthink through‚Äù the language itself rather than the content.

- **Ambiguous Pronouns and References:** Unclear referents (e.g. ‚ÄúSend the report to **them** when **it** is ready‚Äù with multiple possible referents for ‚Äúthem‚Äù or ‚Äúit‚Äù) can confuse an AI just as they do humans. Unlike humans, the AI can‚Äôt ask for clarification, so it may guess ‚Äì a pathway to mistakes. This is analogous to poorly written procedure steps in which pronouns cause confusion about what object to use (a known documentation bug in high-risk fields). If a system prompt or user prompt uses pronouns without clear antecedents, Claude might latch onto an incorrect assumption from earlier text, leading to a wrong action. **Clarity in references** is key; its absence is a negative trait. As one military writing guide notes, *‚Äúensure each paragraph is coherent; pronouns should clearly refer to the intended antecedent‚Äù* [Army 2020]. Violating this principle in LLM prompts can yield indeterminate outputs (the model‚Äôs answer might subtly shift if its internal resolution of a pronoun changes).

- **Double Negatives and Conditional Caveats:** Complex negations (e.g. ‚ÄúDo not fail to include‚Ä¶‚Äù) and stacked conditions make it easy for the model to slip. A prompt that says ‚ÄúAvoid not addressing X‚Äù is grammatically confusing ‚Äì does it mean ‚Äúaddress X‚Äù plainly? Ambiguously negated phrasing increases the risk of the model misunderstanding the requirement (possibly doing the opposite). In cognitive terms, negation adds processing load and is prone to errors in both human and AI interpretation [Sweller 2011]. Similarly, instructions loaded with conditional clauses (‚ÄúIf A, unless B, then do C‚Äù) may be handled incorrectly if the model misses one condition while focusing on another. Each extra clause is a chance for a **misparse or dropped clause** in generation. Clarity suffers greatly with such constructs, which is why simplified logical structures or bullet lists are recommended in technical writing instead of burying logic in one sentence [ISO 2022].

**In summary,** linguistic pitfalls revolve around *ambiguity and density*. Good instructions use straightforward, active language; bad ones use convoluted or equivocal wording. Claude‚Äôs compliance depends on unambiguous cues ‚Äì any fuzziness in language (hedging, passive voice, legalese, or run-on syntax) can degrade determinism and accuracy. Empirical studies confirm that *‚Äúpoor writing drives processing difficulty‚Äù* more than complex content itself [oai_citation:12‚Ä°loweringthebar.net](https://www.loweringthebar.net/2022/09/study-bad-writing-makes-legal-documents-hard-to-read.html#:~:text=Read%20www,account%20of%20legal%20theory). Prompts full of these negative traits thus invite errors or require extra tokens as the model ‚Äúthinks‚Äù its way through the confusion.

## 4. Cognitive Load Drivers (Excessive Mental Demand)  
Even when a prompt is structurally and linguistically sound, it can overwhelm an LLM (or a human reader) by imposing **extraneous cognitive load**. Key factors that increase cognitive strain in prompts include:

- **Split Attention Requirement:** This occurs when the prompt‚Äôs format forces the model to divide its attention between multiple sources or parts to understand the instructions. In humans, classic examples are text that must be read alongside a separate diagram, causing mental back-and-forth. In Claude‚Äôs case, a split-attention prompt might present a rule in one paragraph and an important caveat in a distant footnote, requiring the model to hold one in memory while parsing the other. Cognitive Load Theory shows that split attention *significantly hinders understanding* by overloading working memory [Sweller 2011]. If the model has to reconcile far-flung pieces of context, it uses up more of its context window and is more likely to **forget or confuse** details. Thus, prompts that are not self-contained and coherent ‚Äì effectively asking the model to ‚Äúmulti-task‚Äù within the context ‚Äì are a negative design. They lead to higher token usage and greater chance of instructions being overlooked.

- **Extraneous Details and Irrelevant Information:** When a prompt contains a lot of *nice-to-know* background or unrelated text that isn‚Äôt necessary for the task, it creates **extraneous cognitive load**. The model must sift signal from noise. Human factors research indicates that any element not contributing to task goals will distract and reduce overall performance [Evans et al. 2024]. For example, a system instruction that includes paragraphs of general advice or verbose role-play (beyond what‚Äôs needed for the specific query) can derail focus. Claude might spend effort producing a style or detail consistent with the extraneous info rather than excelling at the core task. In practical terms, unnecessary context not only risks hallucinations (the model might incorporate those random details into the answer), but also wastes tokens. High **mental demand** with low relevance is a known recipe for error in user interfaces and procedures [Hart & Staveland 1988]. In prompts, it manifests as the model losing track of the actual question amidst the clutter.

- **High Information Density (Intrinsic Load) Without Chunking:** Some tasks are inherently complex ‚Äì e.g. a prompt might involve multiple pieces of data or a multi-step logical problem. If such intrinsically heavy content is presented without any scaffolding (no bullet points, no numbering, no step-by-step breakdown), the intrinsic cognitive load can exceed the model‚Äôs effective capacity. Cognitive load theory differentiates *intrinsic load* (complexity of the material itself) from extraneous load; good design tries to **segment or scaffold** high intrinsic load material [Sweller 2011]. In a prompt, failing to break a complex task into smaller steps or clear sub-tasks forces the model to hold a lot of details in working memory at once. This often results in the model dropping some constraints or steps when generating an answer ‚Äì a direct parallel to human error when overwhelmed by too much information simultaneously. For instance, a single prompt paragraph describing a complicated scenario with many variables and then asking for an analysis in one go is a likely negative driver: the model might focus on the most salient few details and ignore others, or get parts of the scenario wrong (hallucinating facts that ‚Äúfill in‚Äù for forgotten ones).

- **Redundant or Conflicting Objectives:** If a prompt gives **conflicting goals** or criteria that compete, it increases mental effort as the model tries to reconcile them. An example is instructing, ‚ÄúProvide a detailed explanation, but keep it concise.‚Äù This puts the model in a double bind: ‚Äúdetailed‚Äù vs ‚Äúconcise‚Äù are opposing pulls with no guidance on priority. In human psychology, such competing objectives cause confusion and stress ‚Äì the NASA Task Load Index (TLX) includes *frustration* as a factor that rises when people face contradictory demands [Hart & Staveland 1988]. For Claude, ambiguous optimization targets can lead to unpredictable outputs depending on which side of the trade-off it prioritizes on a given run. Another example: ‚ÄúBe creative but do not deviate from the facts.‚Äù The model might err on one side (either producing dull factual output or hallucinating creative additions) because balancing these adds cognitive load. **Competing instructions in the same block** are thus a negative complexity driver. Ideally, instructions would clarify precedence (e.g. ‚Äúabove all, facts are paramount ‚Äì only add creativity if it doesn‚Äôt introduce fiction‚Äù), but without that, the model is left to its own devices.

- **Lack of Modular Structure (Chunking):** Closely related to the above, if a prompt does not chunk information into digestible pieces, it fails to utilize the model‚Äôs strength in understanding structured input. Research in instructional design shows *chunking information* reduces working memory burden and errors [Evans et al. 2024]. In prompt terms, chunking could mean using lists, numbering, or separate messages to delineate parts of a task. A negative pattern is a monolithic prompt that blends context, instructions, and examples all together. This all-in-one approach makes it hard for the model to distinguish which sentences are background versus which are explicit directives. The result can be misunderstandings (e.g., the model might treat an example as an actual user request, or vice versa). Essentially, a lack of clear separation between context and task in the prompt increases cognitive load and error likelihood. It‚Äôs akin to an unchecked *‚Äúcontinuous text‚Äù* style manual, which technical writers avoid because readers miss important cues when nothing stands out [PLAIN 2011].

In sum, these cognitive-load factors make prompts **harder to process**. A hallmark of good instruction (whether for a pilot, a surgeon, or an AI) is that it minimizes unnecessary mental work so the focus can be on the task. Negative drivers like extraneous info, split attention, and unchunked complexity all do the opposite ‚Äì they squander part of Claude‚Äôs finite attention on managing the prompt itself. Empirical cognitive studies consistently show that when instruction design is poor, *accuracy and speed plummet* as working memory is overwhelmed [oai_citation:13‚Ä°pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC12246501/#:~:text=The%20Application%20of%20Cognitive%20Load,design%2C%20the%20fewer%20cognitive) [oai_citation:14‚Ä°numberanalytics.com](https://www.numberanalytics.com/blog/cognitive-load-in-design#:~:text=Cognitive%20Load%20in%20Design%20,Germane%20Cognitive%20Load). With LLMs, this translates to inefficient use of the context window and increased chances of mistakes or omissions in the output.

## 5. High-Reliability Domain Pitfalls Mapped to Prompts  
Lessons from **military, aviation, and clinical** domains ‚Äì where communication can be life-critical ‚Äì highlight specific failure modes in instructions. Many of these map closely to pitfalls in LLM prompt design:

- **Lack of Standard Phraseology:** In aviation and military comms, everyone adheres to standardized phrases (e.g. ‚Äústand by‚Äù, ‚Äúaffirmative‚Äù) to avoid miscommunication. Deviating from these norms is known to cause errors or delays. For example, using a casual or non-standard phrase over the radio can lead to hesitation as the receiver interprets it, wasting time [oai_citation:15‚Ä°marines.mil](https://www.marines.mil/News/Messages/Messages-Display/Article/3408501/q2-fy23-approved-and-archived-doctrine-and-training-publications/#:~:text=publications%20www,surface%2C). In prompting, an analogy is using an inconsistent style or format. If one prompt says ‚ÄúList the steps:‚Äù and another similar prompt says ‚ÄúCould you perhaps enumerate how one might proceed,‚Äù the model may not consistently recognize they‚Äôre the same type of instruction. **Unstandardized phrasing** ‚Äì i.e. not keeping a consistent prompt style for similar tasks ‚Äì is a negative driver for determinism. It‚Äôs akin to switching jargon in the cockpit; the model might ‚Äúhesitate‚Äù (in effect, yield different outputs or require more tokens to respond). Consistency and predictable cue words help models latch onto the task more reliably. An authoritative example is the Multi-Service Brevity Codes manual, which exists precisely because brevity and uniform terms prevent confusion in joint operations [DoD 2020]. Prompt designers similarly benefit from a consistent vocabulary; its absence injects uncertainty.

- **Overloaded Communications (Too Much, Too Fast):** The concept of *overload* is well-documented in high-risk fields. A field commander who rattles off a long, complex order in one breath risks subordinates missing parts of it ‚Äì hence the need for concise, chunked orders [oai_citation:16‚Ä°trngcmd.marines.mil](https://www.trngcmd.marines.mil/Portals/207/Docs/TBS/B2B2377%20Combat%20Orders%20Foundations.pdf#:~:text=out%20on%20the%20battlefield,troops%20suffered%20much%20from%20the) [oai_citation:17‚Ä°trngcmd.marines.mil](https://www.trngcmd.marines.mil/Portals/207/Docs/TBS/B2B2377%20Combat%20Orders%20Foundations.pdf#:~:text=delays%20involved%20in%20preparing%20long,good%20order%20is%20as%20much). In the cockpit, overloaded radio calls have led to missed critical information when pilots were saturated with tasks [FAA 2004]. When mapping this to LLM prompts: a single message that tries to do everything (set context, specify format, enumerate multiple constraints, give examples, and ask the question all at once) is an overloaded communication. It‚Äôs prone to *parts of the instruction being missed or ignored*. Indeed, long prompts with many constraints often see the model satisfy the first few and neglect later ones (a phenomenon noted in recent LLM evaluations [Jiang et al. 2024]). Just as humans use techniques like read-backs and step-by-step directives to cope with overload, an AI prompt should ideally be broken into manageable parts. If it isn‚Äôt, it mirrors known human failure modes under high workload: tasks are dropped or completed incorrectly when the **message density** is too high. Thus, monster one-shot prompts are a negative driver ‚Äì they may exceed Claude‚Äôs internal capacity to faithfully execute every part.

- **Missing Acknowledgment or Feedback Loops:** In crew resource management, one principle is to have call-and-response or confirmation of critical instructions (‚ÄúGear down?‚Äù ‚ÄúGear down confirmed.‚Äù). Without this, misunderstandings go unchecked. Obviously an AI can‚Äôt ask for clarification mid-prompt (unless designed to), but a related pitfall is **assuming the model has certain knowledge or context without confirming it**. For instance, a system instruction might assume the assistant knows it should not produce disallowed content because of an earlier policy statement ‚Äì but if that policy is far back in the system prompt, the model might not apply it unless reminded. This is analogous to not confirming that a team member heard the plan correctly. A negative pattern is when prompts rely on implicit shared context that isn‚Äôt actually reinforced. The WHO surgical checklist was introduced largely to force teams to speak up and verify critical points (correct patient, procedure, etc.), after recognizing that *omissions and assumptions kill* [oai_citation:18‚Ä°wegrowteachers.com](https://wegrowteachers.com/cognitive-load-theory-revisited/#:~:text=Cognitive%20Load%20Theory%20Revisited%20,distracts%20or%20confuses%20learners) [oai_citation:19‚Ä°numberanalytics.com](https://www.numberanalytics.com/blog/cognitive-load-in-design#:~:text=to%20high%20extraneous%20cognitive%20load,Germane%20Cognitive%20Load). Similarly, in prompts, if a critical constraint is merely implied or not re-stated when relevant, Claude might ‚Äúassume all is well‚Äù and proceed incorrectly. The failure to explicitly surface important context or get a virtual acknowledgment (through the model‚Äôs own summary, for example) can lead to instruction failure. In short, **unconfirmed context or missing feedback** in instructions is a risk; it‚Äôs the prompt design equivalent of not having a co-pilot read back an altitude clearance.

- **Multi-Step Tasks Without Clear Order or Ownership:** High-reliability domains stress clear sequencing of tasks and assignment of roles. If an operating room checklist step contains multiple actions for different people (‚ÄúAnesthesiologist and surgeon both verify equipment and patient details‚Äù), confusion can occur about who does what ‚Äì so good checklists separate these concerns [WHO 2009]. In prompting, a similar pitfall is giving a list of instructions that are not clearly delineated or prioritized. For example, a system prompt might say: ‚ÄúYou are a coder and a reviewer. Fix the bug and explain the fix and review the code for style.‚Äù Here multiple roles and objectives are mixed. The model might focus on one role (fixing code) and neglect the other (review), or muddle them together. **Role confusion** and unclear division of labor within a prompt (especially with agentic models that have tools or personas) can degrade performance. Anthropic‚Äôs own guidance suggests giving Claude a clear role in system prompts for a reason ‚Äì a lack of a clear role or multiple competing roles in the instructions can cause the model to oscillate or produce erratic outputs [Anthropic 2023]. Thus, prompts should avoid the ‚Äúeveryone does everything‚Äù trap that real teams also avoid. When we see instructions that don‚Äôt specify *who* (model vs user vs tool) handles which part of a task, that‚Äôs a red flag for potential failure.

- **Loss of Shared Mental Model (Context Disconnect):** In team operations, everyone needs the same situational awareness. Orders often start with a situation summary precisely to align mental models [oai_citation:20‚Ä°trngcmd.marines.mil](https://www.trngcmd.marines.mil/Portals/207/Docs/TBS/B2B2377%20Combat%20Orders%20Foundations.pdf#:~:text=communicate%20your%20plan,In%20the%20Student%20Handout). A known failure mode is when sub-units act on outdated or partial information because the orders didn‚Äôt convey the full context. In LLM terms, a prompt that is incomplete or assumes too much can cause the model to fill gaps with its own training priors. For example, if a financial analysis prompt doesn‚Äôt specify the region or timeframe and Claude assumes standard defaults, the result might conflict with user expectations. The **situational picture** wasn‚Äôt fully shared. Another case is when the system prompt context and the user prompt context diverge (perhaps due to a long chat history or an updated instruction that wasn‚Äôt reconciled with earlier ones). If the model doesn‚Äôt ‚Äúrealize‚Äù a change in context because instructions were not explicit, it‚Äôs akin to a crew not realizing the mission parameters changed ‚Äì a classic recipe for error. Maintaining a **consistent context within the prompt** is crucial; failure to do so (through oversight or poor updates in a conversation) is a complexity driver that can lead to hallucinations or irrelevant answers. The model is essentially acting on an outdated mental model. High-reliability practice would call for a formal update or restatement of context whenever it changes; prompts should do the same. When they don‚Äôt, we have the negative pattern of context drift.

These domain-inspired pitfalls underscore that many **instruction failures are predictable and preventable**. Whether it‚Äôs an airplane cockpit or a chat with Claude, unclear communication leads to mistakes. Military and clinical checklists evolved to eliminate exactly things like ambiguous wording, overloaded steps, missing confirmations, and unclear responsibilities [oai_citation:21‚Ä°wegrowteachers.com](https://wegrowteachers.com/cognitive-load-theory-revisited/#:~:text=Cognitive%20Load%20Theory%20Revisited%20,distracts%20or%20confuses%20learners) [oai_citation:22‚Ä°trngcmd.marines.mil](https://www.trngcmd.marines.mil/Portals/207/Docs/TBS/B2B2377%20Combat%20Orders%20Foundations.pdf#:~:text=out%20on%20the%20battlefield,troops%20suffered%20much%20from%20the). The presence of those traits in an LLM prompt strongly indicates a higher risk of the model erring or producing inefficient output. This cross-disciplinary convergence gives us confidence in tagging these as ‚Äúred flag‚Äù characteristics for prompt design.

## 6. Evidence from 2024‚Äì2025 AI Benchmarks (Complexity vs Performance)  
Contemporary research on LLM instruction-following provides empirical backing for the above failure modes. Recent benchmarks specifically stress-tested models like Claude 4 with complex or layered instructions, yielding insights into how **negative complexity drivers** impact performance:

- **Performance Degrades with Added Constraints:** *FollowBench* (ACL 2024) introduced prompts with an increasing number of fine-grained constraints (content requirements, style guides, format specs, etc.) [oai_citation:23‚Ä°aclanthology.org](https://aclanthology.org/2024.acl-long.257.pdf#:~:text=are%3A%20,beyond%20capabilities%20such%20as%20knowledge). The finding: *‚Äúthe performance of all tested models declines substantially with an increase in difficulty level (number of constraints)‚Äù* [oai_citation:24‚Ä°aclanthology.org](https://aclanthology.org/2024.acl-long.257.pdf#:~:text=are%3A%20,beyond%20capabilities%20such%20as%20knowledge). Even top models like GPT-4 or Claude could only reliably satisfy about 3 constraints simultaneously before failing additional ones [oai_citation:25‚Ä°aclanthology.org](https://aclanthology.org/2024.acl-long.257.pdf#:~:text=are%3A%20,more%20challenging%20for%20LLMs%20than). This confirms that instruction overload is real ‚Äì each extra rule or detail is a potential negative driver. The observation aligns with our analysis that multi-topic or overly prescriptive prompts cause omissions. Notably, certain constraint types were especially challenging: **situational context constraints** (adding lots of background info) and **example-driven constraints** (providing an example and requiring the model to follow its pattern) led to more failures [oai_citation:26‚Ä°aclanthology.org](https://aclanthology.org/2024.acl-long.257.pdf#:~:text=satisfy%20around%20three%20constraints%20on,beyond%20capabilities%20such%20as%20knowledge). The ‚ÄúSituation‚Äù result highlights that extensive context can backfire (the model gets distracted or confused by background details), and the ‚ÄúExample‚Äù result suggests that embedding example data in the prompt can mislead the model if not very clear. Both are essentially instances of extraneous cognitive load and split attention hurting accuracy [Jiang et al. 2024].

- **Mixed or Composite Instructions Reveal Deficiencies:** *ComplexBench* (NeurIPS 2024) evaluated instructions composed of multiple constraint types simultaneously (e.g. a query that requires factual correctness, a specific format, and a particular style all at once) [oai_citation:27‚Ä°arxiv.org](https://arxiv.org/abs/2407.03978#:~:text=mainly%20focus%20on%20modeling%20different,verify%20whether%20generated%20texts%20can) [oai_citation:28‚Ä°arxiv.org](https://arxiv.org/abs/2407.03978#:~:text=evaluation%20score%20based%20on%20the,instructions%20with%20multiple%20constraints%20composition). It identified *‚Äúsignificant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition.‚Äù* [oai_citation:29‚Ä°arxiv.org](https://arxiv.org/abs/2407.03978#:~:text=evaluation%20score%20based%20on%20the,instructions%20with%20multiple%20constraints%20composition). In other words, when an instruction has different kinds of requirements that interact (the kind of scenario that often leads to conflicting or dense prompts), models struggle. For instance, a prompt might say: ‚ÄúAnswer this question using information from the text (knowledge constraint), in the form of a bullet-point list (format constraint), and in a humorous tone (style constraint).‚Äù Such composite prompts exhibited failure modes like the model satisfying one aspect (e.g. format) but not the other (dropping the humor, or vice versa). This is empirical confirmation of the **‚Äúcurse of multiple instructions‚Äù** [Wen et al. 2024]. Each added dimension increases cognitive load and chance of conflict, aligning with our earlier point that competing or numerous objectives are a negative driver. ComplexBench‚Äôs taxonomy of 19 constraint dimensions also helps pinpoint what combinations are worst ‚Äì e.g., combining a knowledge-intensive task with a strict format was especially error-prone [Wen et al. 2024]. This suggests that prompts which simultaneously demand domain reasoning and rigid formatting (a not uncommon scenario in system instructions) are likely to trip up Claude 4.

- **Stability Issues in Long Prompts:** *LIFBench* (2024/25) specifically looked at **long-context scenarios**, measuring how consistently models follow instructions as the input context grows in size [oai_citation:30‚Ä°arxiv.org](https://arxiv.org/abs/2411.07037#:~:text=,based%20assessment%20method%20that%20enables) [oai_citation:31‚Ä°arxiv.org](https://arxiv.org/abs/2411.07037#:~:text=analysis%20of%20model%20performance%20and,settings%2C%20offering%20valuable%20insights%20to). Findings from LIFBench indicate that as prompt length increases (spanning thousands of tokens), models like Claude become less stable in following the given instructions [Wu et al. 2024]. They may start ignoring earlier instructions or crucial details ‚Äì a phenomenon akin to a human forgetting earlier parts of a long briefing. This supports the idea that long, unsegmented prompts (one of our structural negatives) are problematic. LIFBench also introduced an automated scoring for *stability*, highlighting that even if a model can handle short instructions well, its performance can drift when those instructions are embedded in a very long conversation or document. In practical terms: if a system prompt plus user prompt together are extremely lengthy, Claude‚Äôs likelihood of strict compliance drops. This is evidence for **context dilution** as a driver of errors ‚Äì important instructions drown in the sea of context if not reinforced. It underscores why keeping prompts concise and focused (to minimize unnecessary token load) is vital; long-winded instructions invite model lapses.

- **Knowledge vs Instruction Conflicts:** A 2024 study on *Knowledge-Conditioned Instruction Following (KCIF)* observed that LLMs often **struggle when instructions tell them to modify or filter their knowledge-based answer** [Murthy et al. 2024]. For example, if an instruction says ‚ÄúFrom the following facts, give an answer *without mentioning X*,‚Äù models frequently slip and mention X anyway, especially if X is highly relevant or salient in their knowledge. This reveals a failure mode where the model‚Äôs internal knowledge retrieval competes with the explicit instruction ‚Äì a kind of ‚Äúextraneous load‚Äù where the model is juggling two drives (truthfully output knowledge vs. obey formatting/content rules). Murthy et al. found that even simple modifying instructions (like ‚Äúanswer in one sentence‚Äù or ‚Äúdon‚Äôt use proper nouns‚Äù) are missed at notable rates when they go against the grain of the content [Murthy et al. 2024]. This empirically backs the notion that **ambiguous or secondary instructions** can be dropped. If the prompt doesn‚Äôt strongly emphasize a constraint, the model‚Äôs default behavior (e.g. giving a thorough answer including all facts) might override it. We can relate this to our observation that hedged or buried constraints are risky ‚Äì the model might simply ignore a softly phrased ‚Äúplease avoid X‚Äù if its knowledge prior deems X important. The KCIF results encourage prompt writers to assume that *any instruction which competes with salient knowledge needs to be extremely clear and perhaps repeated.* If not, it‚Äôs a prime point of failure.

- **‚ÄúCurse of Multiple Instructions‚Äù:** A 2025 report bluntly titled *‚ÄúLarge Language Models Cannot Follow Multiple Instructions at Once‚Äù* [Harada et al. 2025] reinforces many of the above points. The authors characterize a phenomenon where even state-of-the-art models break down when given too many or concurrent instructions. They note that current LLMs are *inevitably prone to failure if asked to satisfy numerous constraints simultaneously*, calling it a fundamental limitation in present architectures [Harada et al. 2025]. This is essentially a direct warning that piling on instructions ‚Äì especially those that might conflict or require trade-offs ‚Äì will likely yield incomplete compliance. It validates our cross-domain intuition: just as a single person can only effectively track a handful of tasks at once, an LLM has a limit to how many independent instructions it can juggle in one prompt. Beyond that, it will drop something important or muddle them. The ‚Äúcurse‚Äù paper suggests this is an ‚Äúextremely inconvenient fact‚Äù for practical LLM use, meaning prompt engineers must be mindful of not overloading a prompt. This research provides a strong theoretical backing to label **multi-instruction prompts as high risk** ‚Äì an accumulation of negative drivers often leads to outright failure.

In summary, the cutting-edge evaluations of 2024‚Äì2025 strongly corroborate the idea that **increasing complexity negatively impacts LLM performance** across the board. Each additional constraint, each extra paragraph of context, each simultaneous objective incrementally raises the chance of error or non-compliance. These studies give quantified evidence to what earlier sections described qualitatively. They also highlight specific trouble spots: contextual distractions, format/style requirements in tension with content, and sheer number of instructions are all common culprits when models go astray [oai_citation:32‚Ä°aclanthology.org](https://aclanthology.org/2024.acl-long.257.pdf#:~:text=are%3A%20,beyond%20capabilities%20such%20as%20knowledge) [oai_citation:33‚Ä°arxiv.org](https://arxiv.org/abs/2407.03978#:~:text=evaluation%20score%20based%20on%20the,instructions%20with%20multiple%20constraints%20composition). For a Claude 4 Opus user, these findings translate to a simple mandate: keep prompts as straightforward as possible. The research shows that *even the best LLM today has a fragile ability to follow complex, lengthy, or conflicting instructions*. Recognizing the negative drivers in prompt design is the first step to avoiding them.

## 7. Examples of Problematic Instructions (Illustrative Excerpts)  
To concretize these abstract failure modes, here are **anonymized prompt snippets** that exhibit one or more negative complexity drivers. Each example is paired with a brief note on why it‚Äôs problematic for Claude 4. (These are ‚Äúwhat not to do‚Äù illustrations ‚Äì they highlight issues without showing the corrected prompt.)

- **Example 1: Buried Main Request**  
  *Prompt:* ‚ÄúHello Claude, welcome back! üôÇ Before we start, let me remind you of some background: [two paragraphs of context]. Anyway, I was thinking, if it isn‚Äôt too much trouble, perhaps you could assist me with something related to the data we discussed. The figures from Q3, which, as you know, might be slightly skewed due to seasonal effects, need to be reevaluated. We should ensure no anomalies. So, yeah, could you do that analysis when you have a chance?‚Äù  
  **Issue:** The actual task (‚Äúdo that analysis‚Äù of Q3 figures) is **hidden at the very end** and phrased vaguely. The prompt spends dozens of tokens on greetings and context and only indirectly asks for analysis. This exemplifies a buried lead and hedging language (‚Äúperhaps‚Ä¶ when you have a chance‚Äù) [Army 2013; PLAIN 2011]. Claude might produce a friendly chatty reply or summarize the background instead of performing the analysis, because the instruction was not prominent or clear. 

- **Example 2: Overloaded Multi-Task Directive**  
  *Prompt:* ‚Äú1. Summarize the following article in detail. 2. Translate the summary to French. 3. List five insights from the article. 4. Write a critique of the article‚Äôs methodology. 5. All outputs must be in a JSON format with clearly labeled sections in markdown.‚Äù  
  **Issue:** Five distinct tasks and an additional format requirement are given at once, with no prioritization. This is an extreme case of **overloading and competing objectives** [Harada et al. 2025]. The model might execute the first few and ignore the rest, or mix up formats (e.g. forgetting to use JSON for the critique). Each item is complex on its own; together they‚Äôre very likely to exceed the model‚Äôs constraint-following capacity (as per FollowBench‚Äôs findings of failures beyond ~3 simultaneous constraints [oai_citation:34‚Ä°aclanthology.org](https://aclanthology.org/2024.acl-long.257.pdf#:~:text=are%3A%20,more%20challenging%20for%20LLMs%20than)). The lack of any guidance on how to balance detail vs. brevity (especially for the summary vs. insights vs. critique) adds to the cognitive load. 

- **Example 3: Ambiguous Reference and Passive Voice**  
  *Prompt:* ‚ÄúThe user‚Äôs query was processed by the system. It produced results that may be relevant, but they have not been reviewed. Provide an answer based on this, ensuring it is comprehensive. They should be notified if any assumptions were made.‚Äù  
  **Issue:** This instruction is in **passive voice** and has unclear pronouns. Who ‚Äúproduced results‚Äù ‚Äì the system or Claude? Who is ‚Äúthey‚Äù that should be notified ‚Äì the user or someone else? The request to ‚Äúprovide an answer based on this‚Äù is vague (based on what exactly ‚Äì the unreviewed results?). The passive construction ‚Äúwas processed‚Äù hides the agent, and ‚Äúthey should be notified‚Äù is an unclear obligation [Army 2013]. Claude could easily misinterpret who does what, possibly resulting in an answer that speaks to the wrong audience or fails to notify the user. The prompt‚Äôs lack of clarity mirrors the kind of ambiguous instruction that in aviation CRM would prompt a ‚ÄúWho, me? Did you mean me to do that?‚Äù ‚Äì but the AI won‚Äôt ask, it will guess (risking error).

- **Example 4: Excessive Context and Split Attention**  
  *Prompt:* ‚Äú[System note: The assistant is a financial expert AI in 2025, with knowledge of global markets.] User: Given the data below, predict next quarter‚Äôs trends. [Attached is a 5-page financial report with dense tables and text] Also, recall the principles of fiscal policy we discussed earlier, and the latest news about central bank rates. Make sure to format the answer as specified in the company style guide provided in the PDF.‚Äù  
  **Issue:** This is a **context soup**. It provides a role, a broad request, a huge chunk of data, references another discussion (‚Äúdiscussed earlier‚Äù) and external info (‚Äúlatest news‚Äù), plus a formatting rule buried at the end referring to yet another document. The model has to *split attention* across multiple sources: the user query, the financial report, memory of fiscal policy principles, current events knowledge, and a style guide. This exemplifies extraneous load and split-source integration [Sweller 2011]. Claude may either ignore some inputs (e.g. skip the style guide or forget the news) or produce a very lengthy response trying to satisfy everything, perhaps timing out or losing coherence. Each additional context source is a potential point of failure ‚Äì the prompt lacks focus. A human facing such instruction would likely be overwhelmed; the AI likewise may falter or use tokens inefficiently trying to juggle it all.

- **Example 5: Conflicting Tone and Content Guidance**  
  *Prompt:* ‚ÄúGenerate a response to the user that is extremely thorough and covers every detail. It should be written in a very simple manner that a child could understand. Use a formal academic tone throughout. Keep it brief.‚Äù  
  **Issue:** This prompt gives **conflicting directives** on style and length. ‚ÄúExtremely thorough‚Äù vs. ‚Äúkeep it brief‚Äù is an obvious contradiction, as is ‚Äúchild-understandable‚Äù vs. ‚Äúformal academic tone.‚Äù The model cannot satisfy all these simultaneously ‚Äì it‚Äôs forced into an internal conflict [Hart & Staveland 1988]. This will likely yield an unpredictable compromise: e.g. a moderately detailed answer with semi-formal tone, which might not actually please any of the criteria. Worse, the evaluation of success is unclear: if the output is brief, it fails thoroughness; if it‚Äôs detailed, it fails brevity. This no-win situation is a classic *instruction failure mode*. It exemplifies why prompts should avoid incompatible goals. In essence, it‚Äôs setting Claude up to ‚Äúlose‚Äù no matter what it does, which is evident in inconsistent outputs across attempts.

Each of the above problematic examples embodies one or more negative complexity drivers identified earlier. Spotting these patterns in your own prompts can warn you that Claude 4 might misinterpret or partially comply. In the next section, we distill such warnings into a handy checklist.

## 8. Red-Flag Checklist for Prompt Complexity Pitfalls  
When authoring prompts or system instructions for Claude 4 Opus (or similar LLMs), use this **checklist of red flags** to quickly assess if your prompt might suffer from negative complexity drivers. If you check any of these boxes, consider that a likely source of reduced accuracy or efficiency in the model‚Äôs response:

- **Main point hidden or delayed?** ‚Äì Does the prompt fail to state the primary request *up front* in clear terms? (BLUF violation is a red flag for confusion) [Army 2013].

- **Paragraphs overly long or unstructured?** ‚Äì Do you have large blocks of text without breaks, lists, or emphasis? (Wall-of-text prompts overload the model‚Äôs working memory) [Army 2013].

- **Inconsistent or confusing organization?** ‚Äì Are similar items grouped, and does the prompt flow logically? If headings or bullets jump around without logic, the model may lose the thread (structure should mirror the task order) [ISO 2022].

- **Multiple tasks jumbled together?** ‚Äì Are you asking for several distinct outputs in one prompt (e.g. summarize *and* critique *and* translate)? If so, the model might complete only some. Split them if possible ‚Äì too many simultaneous tasks is a known failure trigger [Jiang et al. 2024].

- **Passive voice or unclear agents?** ‚Äì Check for ‚Äúis/was done‚Äù phrasing. Does the prompt clearly say who should do what (the AI, the user, a function)? If the actor is implicit or sentences are passive, rewrite actively to avoid ambiguity [Dept. of the Army 2013].

- **Hedging or non-committal language?** ‚Äì Are there words like ‚Äúmaybe, if you can, sort of, perhaps‚Äù diluting an instruction? These can make the model treat a requirement as optional. Prefer direct language (‚Äúdo X‚Äù) for critical steps [PLAIN 2011].

- **Undefined jargon or acronyms?** ‚Äì Is any term or abbreviation used that the model or a new reader might not know immediately? If yes, define it or simplify. Don‚Äôt assume context ‚Äì unclear jargon invites hallucination or errors [Plain Language 2011].

- **Long, complex sentences?** ‚Äì Do you have sentences packed with multiple clauses or parentheticals? If a sentence is hard to read aloud in one breath, it‚Äôs probably hard for the model too. Break it into simpler sentences to reduce parse difficulty [Mart√≠nez et al. 2022].

- **Ambiguous references (this/it/they)?** ‚Äì Do pronouns or pointers in the prompt have a single obvious antecedent? If not, clarify them. For instance, change ‚Äúwhen it is done, send it‚Äù to ‚Äúwhen the analysis is done, send the report to the user‚Äù [Army 2020].

- **Extraneous information included?** ‚Äì Does the prompt contain background or details that aren‚Äôt directly needed to produce the answer? If so, consider removing or moving them out of the immediate instruction. Extra info can distract the model (extraneous load) [Sweller 2011].

- **Context requires cross-referencing?** ‚Äì Are important pieces of the instruction separated such that the model must ‚Äújump‚Äù between parts of the prompt (like an attached document, or widely separated paragraphs)? If yes, try to consolidate or at least remind the model of them at execution point. Split attention is a known pitfall [Sweller 2011].

- **Conflicting or dual objectives?** ‚Äì Does the prompt ask for two things that may be at odds (e.g. ‚Äúbe creative but don‚Äôt add anything new‚Äù)? Resolve the conflict or set a clear priority. Conflicting instructions often lead to random compliance or mixed results [Hart & Staveland 1988].

- **No clear sequence for multi-step tasks?** ‚Äì If the task has several steps, are they enumerated or clearly ordered? If you just lump them in one paragraph, the model might skip or reorder them. Use lists or step numbers for multi-step processes [WHO 2009].

- **Role or perspective confusion?** ‚Äì If the prompt involves roles (system vs user vs assistant or multiple personas), is it clear who is ‚Äúspeaking‚Äù or being referred to at each point? Ambiguity in role can cause the model to answer from the wrong perspective or leak system text. Ensure role indicators (like ‚ÄúAssistant:‚Äù or persona descriptions) are unambiguous [Anthropic 2023].

- **Overly long overall prompt?** ‚Äì Is your prompt nearing the model‚Äôs context length, or simply very verbose? If so, trim unnecessary parts. Long prompts especially with instructions at the beginning can lead to the model forgetting earlier directives by the time it answers [Wu et al. 2024].

- **Too many constraints or rules?** ‚Äì Count the distinct rules/criteria you‚Äôre imposing (format, style, content, length, etc.). If there are a lot (more than 3‚Äì4), consider which are truly necessary. Each added rule raises the chance the model ignores one [Jiang et al. 2024].

Use this checklist during prompt drafting or review. If you find one of these red-flag issues, revising the prompt to eliminate it will likely improve Claude‚Äôs performance (or at least prevent a predictable failure). Remember, the goal is not to *fix* in this document, but to recognize ‚Äì and the above checks encapsulate the most common negative drivers identified by both experience and research.

